{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "pocsynapseworkspacesa"
		},
		"pocsynapseworkspacesa-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'pocsynapseworkspacesa-WorkspaceDefaultSqlServer'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:pocsynapseworkspacesa.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"synapse8heknbv-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'synapse8heknbv-WorkspaceDefaultSqlServer'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:synapse8heknbv.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"synapsesmgxwkq-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'synapsesmgxwkq-WorkspaceDefaultSqlServer'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:synapsesmgxwkq.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"pocsynapseworkspacesa-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://databrickspocsadab.dfs.core.windows.net"
		},
		"synapse8heknbv-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://datalake8heknbv.dfs.core.windows.net"
		},
		"synapsesmgxwkq-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://datalakesmgxwkq.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/Products_Csv')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "synapsesmgxwkq-WorkspaceDefaultStorage",
					"type": "LinkedServiceReference"
				},
				"annotations": [],
				"type": "DelimitedText",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": "products.csv",
						"folderPath": "products",
						"fileSystem": "files"
					},
					"columnDelimiter": ",",
					"escapeChar": "\\",
					"firstRowAsHeader": true,
					"quoteChar": "\""
				},
				"schema": [
					{
						"name": "ProductID",
						"type": "String"
					},
					{
						"name": "ProductName",
						"type": "String"
					},
					{
						"name": "Category",
						"type": "String"
					},
					{
						"name": "ListPrice",
						"type": "String"
					}
				]
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/synapsesmgxwkq-WorkspaceDefaultStorage')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/pocsynapseworkspacesa-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('pocsynapseworkspacesa-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/pocsynapseworkspacesa-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('pocsynapseworkspacesa-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/synapse8heknbv-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('synapse8heknbv-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/synapse8heknbv-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('synapse8heknbv-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/synapsesmgxwkq-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('synapsesmgxwkq-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/synapsesmgxwkq-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('synapsesmgxwkq-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LoadProductsData')]",
			"type": "Microsoft.Synapse/workspaces/dataflows",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "MappingDataFlow",
				"typeProperties": {
					"sources": [
						{
							"dataset": {
								"referenceName": "Products_Csv",
								"type": "DatasetReference"
							},
							"name": "ProductsFile",
							"description": "Products File Data"
						}
					],
					"sinks": [
						{
							"dataset": {
								"referenceName": "Products_Csv",
								"type": "DatasetReference"
							},
							"name": "ProductDataSink"
						}
					],
					"transformations": [],
					"scriptLines": [
						"source(output(",
						"          ProductID as string,",
						"          ProductName as string,",
						"          Category as string,",
						"          ListPrice as decimal(10,0)",
						"     ),",
						"     allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     ignoreNoFilesFound: false) ~> ProductsFile",
						"ProductsFile sink(allowSchemaDrift: true,",
						"     validateSchema: false,",
						"     input(",
						"          ProductID as string,",
						"          ProductName as string,",
						"          Category as string,",
						"          ListPrice as string",
						"     ),",
						"     umask: 0022,",
						"     preCommands: [],",
						"     postCommands: [],",
						"     skipDuplicateMapInputs: true,",
						"     skipDuplicateMapOutputs: true) ~> ProductDataSink"
					]
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/datasets/Products_Csv')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Alter External Location')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "\n/*\nTo alter the location of an External Data Source in Azure SQL Database or SQL Server, there isn't a direct ALTER \nstatement for it. Instead, you would need to drop the existing external data source and recreate it with the new location.\n*/\nUSE Sales;\nGO;\n    \n\nDROP EXTERNAL DATA SOURCE sales_data;\nGO\n\nCREATE EXTERNAL DATA SOURCE sales_data\nWITH (\n    LOCATION = 'https://datalake8heknbv.dfs.core.windows.net/files/'\n);\nGO\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "Sales",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create External DB')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": " -- Database for sales data\n CREATE DATABASE Sales\n   COLLATE Latin1_General_100_BIN2_UTF8;\n GO;\n    \n Use Sales;\n GO;\n    \n -- External data is in the Files container in the data lake\n CREATE EXTERNAL DATA SOURCE sales_data WITH (\n     LOCATION = 'https://datalake8heknbv.dfs.core.windows.net/OutputFiles/SalesData'\n );\n GO;\n    \n -- Format for table files\n CREATE EXTERNAL FILE FORMAT ParquetFormat\n     WITH (\n             FORMAT_TYPE = PARQUET,\n             DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'\n         );\n GO;",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create External Tbl')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": " USE Sales;\n GO;\n \n CREATE EXTERNAL TABLE ProductSalesTotals\n     WITH (\n         LOCATION = 'sales/productsales/',\n         DATA_SOURCE = sales_data,\n         FILE_FORMAT = ParquetFormat\n     )\n AS\n SELECT Item AS Product,\n     SUM(Quantity) AS ItemsSold,\n     ROUND(SUM(UnitPrice) - SUM(TaxAmount), 2) AS NetRevenue\n FROM\n     OPENROWSET(\n         BULK 'sales/csv/*.csv',\n         DATA_SOURCE = 'sales_data',\n         FORMAT = 'CSV',\n         PARSER_VERSION = '2.0',\n         HEADER_ROW = TRUE\n     ) AS orders\n GROUP BY Item;",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Create_StreamdataTable')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- Column names should align with the Stream Query\n\nCREATE TABLE StreamAnalyticsPOC\n(\n    Customer_ID INT,\n    Full_Name VARCHAR(100),\n    Email_Address VARCHAR(255),\n    EventProcessedUtcTime DATETIME\n)\n\n\nCREATE TABLE StreamSales (\n    StoreName NVARCHAR(100) NOT NULL,\n    StoreLocation NVARCHAR(255) NULL,\n    ProductName NVARCHAR(255) NOT NULL,\n    ProductCategory NVARCHAR(100) NULL,\n    Manufacturer NVARCHAR(255) NULL,\n    CustomerName NVARCHAR(255) NULL,\n    CustomerLoyaltyPoints INT NULL,\n    CustomerLocation NVARCHAR(255) NULL,\n    RegistrationDate DATE NOT NULL,\n    SaleDate DATE NOT NULL,\n    Quantity INT NOT NULL,\n    UnitPrice DECIMAL(18, 2) NOT NULL,\n    TotalPrice DECIMAL(18, 2) NOT NULL,\n    PaymentMethod NVARCHAR(50) NULL,\n    EventProcessedUtcTime DATETIME NOT NULL\n);\n\n\n\n\n\n\n\n\n\n\n\n/* \n\n-- Explored on Stream using some sample data\n\n\nCREATE TABLE StreamSalesData (\n    SaleID BIGINT NOT NULL,\n    StoreName NVARCHAR(255) NOT NULL,\n    ProductName NVARCHAR(255) NOT NULL,\n    CustomerName NVARCHAR(255) NOT NULL,\n    SaleDate DATETIME NOT NULL,\n    Quantity BIGINT NOT NULL,\n    TotalPrice FLOAT NOT NULL,\n    PaymentMethod NVARCHAR(50) NOT NULL\n)\nWITH\n(\n    DISTRIBUTION = HASH(SaleID)  -- Distributes rows across nodes based on SaleID\n);\n\n\n-------------------------------\n\n\n\n\nCREATE TABLE StreamSalesData (\n    SaleID NVARCHAR(255) NOT NULL,\n    StoreName NVARCHAR(255) NOT NULL,\n    ProductName NVARCHAR(255) NOT NULL,\n    CustomerName NVARCHAR(255) NOT NULL,\n    SaleDate DATETIME NOT NULL,\n    Quantity NVARCHAR(255) NOT NULL,\n    TotalPrice NVARCHAR(255) NOT NULL,\n    PaymentMethod NVARCHAR(50) NOT NULL\n)\nWITH\n(\n    DISTRIBUTION = HASH(SaleID)  -- Distributes rows across nodes based on SaleID\n);\n\n\nDROP TABLE StreamSalesData;\n\n\n\n\n\nCREATE TABLE StreamSalesDataDirect\n(\n    SaleID INT, \n    StoreID INT,\n    StoreName VARCHAR(100),\n    ProductID INT, \n    CustomerID INT, \n    SaleDate DATETIME, \n    Quantity INT, \n    UnitPrice DECIMAL(10,2), \n    TotalPrice  DECIMAL(10,2), \n    PaymentMethod VARCHAR(100), \n    EventProcessedUtcTime DATETIME\n)\nWITH\n(\n    DISTRIBUTION = HASH(SaleID)  -- Distributes rows across nodes based on SaleID\n);\n\n\nDROP TABLE StreamSalesDataDirect;\n\n*/\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "Datawarehouse_poc",
						"poolName": "Datawarehouse_poc"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DeltaTableFromServerlessSQL')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- This is auto-generated code\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://datalakesmgxwkq.dfs.core.windows.net/files/delta/products-delta/',\n        FORMAT = 'DELTA'\n    ) AS [result]\n\n\n\n-- Serverless SQL Pool can be used to query delta lake data in catalog tables\n-- that are defined in the Spark Metastore\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "adventureworks",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/External Objects Creation')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- Sample Queries\n\n\n--  used to create the Credential\nCREATE DATABASE SCOPED CREDENTIAL storagekeycred\nWITH\n    IDENTITY='SHARED ACCESS SIGNATURE',  \n    SECRET = 'sv=xxx...';\n\n-- Create a external data source using the Above created credentials\nCREATE EXTERNAL DATA SOURCE secureFiles\nWITH (\n    LOCATION = 'https://mydatalake.blob.core.windows.net/data/secureFiles/'\n    CREDENTIAL = storagekeycred\n);\n\n\n-- create the file format\nCREATE EXTERNAL FILE FORMAT ParquetFormat\nWITH (\n        FORMAT_TYPE = PARQUET,\n        DATA_COMPRESSION = 'org.apache.hadoop.io.compress.SnappyCodec'\n    );\n\n\n-- create a External table using the Data Source Created\nCREATE EXTERNAL TABLE SpecialOrders\n    WITH (\n        -- details for storing results\n        LOCATION = 'special_orders/',\n        DATA_SOURCE = files,\n        FILE_FORMAT = ParquetFormat\n    )\nAS\nSELECT OrderID, CustomerName, OrderTotal\nFROM\n    OPENROWSET(\n        -- details for reading source files\n        BULK 'sales_orders/*.csv',\n        DATA_SOURCE = 'files', -- created Data source\n        FORMAT = 'ParquetFormat', -- Created file format\n        PARSER_VERSION = '2.0',\n        HEADER_ROW = TRUE\n    ) AS source_data\nWHERE OrderType = 'Special Order';\n\n\n-- Droping the External table\nDROP EXTERNAL TABLE SpecialOrders;\n\n/*\nexternal tables are a metadata abstraction over the files that contain the actual data. \nDropping an external table does not delete the underlying files.\n*/\n\n/*\nWhile you can run a CREATE EXTERNAL TABLE AS SELECT (CETAS) statement in a script whenever you need to transform data, \nit's good practice to encapsulate the transformation operation in stored procedure. \nThis approach can make it easier to operationalize data transformations by enabling you to supply parameters,\n retrieve outputs, and include additional logic in a single procedure call.\n */\n\n\n -- Stored procedure Syntax:\n\n /*\n\nCREATE PROCEDURE procedure_name\n@parameters \nAS\nBEGIN\n\n    # Statements to be executed\n\nEND\n\n\n */\n\n\n/*\n\nPROCEDURE USAGE AND ADVANTAGES:\n\nReduces client to server network traffic\nThe commands in a procedure are executed as a single batch of code; which can significantly reduce network traffic between \nthe server and client because only the call to execute the procedure is sent across the network.\n\nProvides a security boundary\nMultiple users and client programs can perform operations on underlying database objects through a procedure, even if the \nusers and programs don't have direct permissions on those underlying objects. The procedure controls what processes and \nactivities are performed and protects the underlying database objects; eliminating the requirement to grant permissions \nat the individual object level and simplifies the security layers.\n\nEases maintenance\nAny changes in the logic or file system locations involved in the data transformation can be applied only to the stored \nprocedure; without requiring updates to client applications or other calling functions.\n\nImproved performance\nStored procedures are compiled the first time they're executed, and the resulting execution plan is held in the cache and \nreused on subsequent runs of the same stored procedure. As a result, it takes less time to process the procedure.\n\n\n*/\n\n\n\n\n\n\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LakeDB_SelectQuery')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": " SELECT o.SalesOrderID, c.EmailAddress, p.ProductName, o.Quantity\n FROM SalesOrder AS o\n JOIN Customer AS c ON o.CustomerId = c.CustomerId\n JOIN Product AS p ON o.ProductId = p.ProductId",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "RetailDB",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/LoadDimAndFactTable')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- Create DimDate Table\nCREATE TABLE DimDate (\n    DateKey INT NOT NULL, -- YYYYMMDD\n    FullDate DATE NOT NULL,\n    DayNumberOfWeek INT,\n    DayNameOfWeek NVARCHAR(50),\n    DayOfMonth INT,\n    MonthName NVARCHAR(50),\n    MonthNumberOfYear INT,\n    Year INT\n)\nWITH (DISTRIBUTION = ROUND_ROBIN);\n\n-- Create DimProduct Table\nCREATE TABLE DimProduct (\n    ProductKey INT NOT NULL,\n    ProductName NVARCHAR(100),\n    ProductCategory NVARCHAR(100),\n    ProductSubcategory NVARCHAR(100),\n    ProductSKU NVARCHAR(50)\n)\nWITH (DISTRIBUTION = HASH(ProductKey));\n\n-- Create DimCustomer Table\nCREATE TABLE DimCustomer (\n    CustomerKey INT NOT NULL,\n    FirstName NVARCHAR(100),\n    LastName NVARCHAR(100),\n    Gender CHAR(1),\n    EmailAddress NVARCHAR(100),\n    PhoneNumber NVARCHAR(15),\n    AddressLine1 NVARCHAR(255),\n    City NVARCHAR(100),\n    State NVARCHAR(50),\n    PostalCode NVARCHAR(20),\n    Country NVARCHAR(50)\n)\nWITH (DISTRIBUTION = HASH(CustomerKey));\n\n-- Create FactSales Table\nCREATE TABLE FactSales (\n    SalesKey BIGINT IDENTITY(1,1) NOT NULL,\n    DateKey INT NOT NULL,\n    ProductKey INT NOT NULL,\n    CustomerKey INT NOT NULL,\n    SalesAmount DECIMAL(18, 2),\n    QuantitySold INT,\n    DiscountAmount DECIMAL(18, 2),\n    TotalAmount DECIMAL(18, 2)\n)\nWITH (DISTRIBUTION = ROUND_ROBIN);\n\n\n\n\n-- Inserting the data\n\n-- Insert Dates from 2024-01-01 to 2199-12-31\nDECLARE @StartDate DATE = '2024-01-01';\nDECLARE @EndDate DATE = '2199-12-31';\n\n-- Generate a series of numbers to represent days\nWITH Numbers AS (\n    SELECT TOP (DATEDIFF(DAY, @StartDate, @EndDate) + 1) \n        ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) - 1 AS Num\n    FROM sys.all_objects\n)\nINSERT INTO DimDate (DateKey, FullDate, DayNumberOfWeek, DayNameOfWeek, DayOfMonth, MonthName, MonthNumberOfYear, Year)\nSELECT \n    CONVERT(INT, CONVERT(VARCHAR(8), DATEADD(DAY, Num, @StartDate), 112)) AS DateKey,\n    DATEADD(DAY, Num, @StartDate) AS FullDate,\n    DATEPART(WEEKDAY, DATEADD(DAY, Num, @StartDate)) AS DayNumberOfWeek,\n    DATENAME(WEEKDAY, DATEADD(DAY, Num, @StartDate)) AS DayNameOfWeek,\n    DATEPART(DAY, DATEADD(DAY, Num, @StartDate)) AS DayOfMonth,\n    DATENAME(MONTH, DATEADD(DAY, Num, @StartDate)) AS MonthName,\n    DATEPART(MONTH, DATEADD(DAY, Num, @StartDate)) AS MonthNumberOfYear,\n    DATEPART(YEAR, DATEADD(DAY, Num, @StartDate)) AS Year\nFROM Numbers;\n\n\n-- Insert Products\n-- Insert Products into DimProduct\nINSERT INTO DimProduct (ProductKey, ProductName, ProductCategory, ProductSubcategory, ProductSKU)\nVALUES (1, 'Laptop', 'Electronics', 'Computers', 'SKU001');\n\nINSERT INTO DimProduct (ProductKey, ProductName, ProductCategory, ProductSubcategory, ProductSKU)\nVALUES (2, 'Smartphone', 'Electronics', 'Mobiles', 'SKU002');\n\nINSERT INTO DimProduct (ProductKey, ProductName, ProductCategory, ProductSubcategory, ProductSKU)\nVALUES (3, 'Tablet', 'Electronics', 'Computers', 'SKU003');\n\nINSERT INTO DimProduct (ProductKey, ProductName, ProductCategory, ProductSubcategory, ProductSKU)\nVALUES (4, 'TV', 'Electronics', 'Home Entertainment', 'SKU004');\n\nINSERT INTO DimProduct (ProductKey, ProductName, ProductCategory, ProductSubcategory, ProductSKU)\nVALUES (5, 'Headphones', 'Electronics', 'Accessories', 'SKU005');\n\n\n\n-- Insert Customers\n-- Insert Customers into DimCustomer\nINSERT INTO DimCustomer (CustomerKey, FirstName, LastName, Gender, EmailAddress, PhoneNumber, AddressLine1, City, State, PostalCode, Country)\nVALUES (1, 'John', 'Doe', 'M', 'john.doe@example.com', '1234567890', '123 Elm St', 'Metropolis', 'CA', '90210', 'USA');\n\nINSERT INTO DimCustomer (CustomerKey, FirstName, LastName, Gender, EmailAddress, PhoneNumber, AddressLine1, City, State, PostalCode, Country)\nVALUES (2, 'Jane', 'Smith', 'F', 'jane.smith@example.com', '0987654321', '456 Oak St', 'Gotham', 'NY', '10001', 'USA');\n\n\n-- Populate FactSales with dummy data\nINSERT INTO FactSales (DateKey, ProductKey, CustomerKey, SalesAmount, QuantitySold, DiscountAmount, TotalAmount)\nSELECT\n    d.DateKey,\n    p.ProductKey,\n    c.CustomerKey,\n    CAST(ABS(CHECKSUM(NEWID()) % 5000 + 1) AS DECIMAL(18, 2)) AS SalesAmount, -- Random Sales Amount\n    ABS(CHECKSUM(NEWID()) % 10 + 1) AS QuantitySold, -- Random Quantity between 1 and 10\n    CAST(ABS(CHECKSUM(NEWID()) % 500) AS DECIMAL(18, 2)) AS DiscountAmount, -- Random Discount\n    (CAST(ABS(CHECKSUM(NEWID()) % 5000 + 1) AS DECIMAL(18, 2)) - CAST(ABS(CHECKSUM(NEWID()) % 500) AS DECIMAL(18, 2))) AS TotalAmount\nFROM DimDate d\nCROSS JOIN DimProduct p\nCROSS JOIN DimCustomer c;\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "Datawarehouse_poc",
						"poolName": "Datawarehouse_poc"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/PostLoadPerformanceOptimization')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "\n-- Statistics Creation\n-- Synapse does not automatically update statistics after data loads. Create or update statistics to help the query optimizer.\n-- For large tables with clustered columnstore indexes, this ensures better query plans.\n\n-- When to Create Statistics: Columns used in Joins, Where, Aggregations\n\n-- Best practices:\n-- Automated Updates: In Synapse SQL pools, statistics are not automatically updated after data loads, so you must explicitly update them, especially after large inserts.\n-- Partitioned Tables: If your fact tables are partitioned (e.g., by date), consider creating statistics on each partition for better query optimization.\n\nCREATE STATISTICS Stat_DateKey\nON FactSales (DateKey);\n\nCREATE STATISTICS Stat_ProductKey\nON FactSales (ProductKey);\n\nCREATE STATISTICS Stat_CustomerKey\nON FactSales (CustomerKey);\n\n-- Rebuild Indexes\n-- If data is inserted into Heap Tables or Clustered Index Tables, rebuilding the indexes can optimize data retrieval.\n\n-- Columnstore Indexes are often the best option for large fact tables in Synapse as they provide both compression and efficient query performance.\n\n-- After loading large amounts of data, consider using the REBUILD or DROP and RECREATE methods to optimize the storage and query performance.\n\nALTER INDEX ALL ON FactSales REBUILD;\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "Datawarehouse_poc",
						"poolName": "Datawarehouse_poc"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Quering_StreamTable')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "SELECT COUNT(1) FROM StreamAnalyticsPOC\n\nSELECT * FROM StreamAnalyticsPOC\nORDER BY Customer_ID \n\n--- Updated POC\n\nSELECT * FROM StreamSales",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "Datawarehouse_poc",
						"poolName": "Datawarehouse_poc"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Query Sales CSV files')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- This is auto-generated code\nSELECT\n    TOP 100 *\nFROM\n    OPENROWSET(\n        BULK 'https://datalake8heknbv.dfs.core.windows.net/files/sales/csv/**',\n        FORMAT = 'CSV',\n        PARSER_VERSION = '2.0',\n        HEADER_ROW = TRUE\n    ) AS [result]",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Select External records')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": " USE Sales;\n GO;\n    \n SELECT Item AS Product,\n        SUM(Quantity) AS ItemsSold,\n        ROUND(SUM(UnitPrice) - SUM(TaxAmount), 2) AS NetRevenue\n FROM\n     OPENROWSET(\n         BULK 'sales/csv/*.csv',\n         DATA_SOURCE = 'sales_data',\n         FORMAT = 'CSV',\n         PARSER_VERSION = '2.0',\n         HEADER_ROW = TRUE\n     ) AS orders\n GROUP BY Item;",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "Sales",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/notes')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "/*\nWith lake databases in Azure Synapse Analytics, you can combine the benefits of both Data Lake and The Relational Database.\n\n**********************************************************\nwhile you can use SQL Database with external tables to reference data in a Data Lake, Lake Database provides a more integrated \nand flexible approach for managing and analyzing large-scale data directly within the data lake environment.\n**********************************************************\n\n\n\n\n*/",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/sp_GetYearlySales')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": " USE Sales;\n GO;\n CREATE PROCEDURE sp_GetYearlySales\n AS\n BEGIN\n     -- drop existing table\n     IF EXISTS (\n             SELECT * FROM sys.external_tables\n             WHERE name = 'YearlySalesTotals'\n         )\n         DROP EXTERNAL TABLE YearlySalesTotals\n     -- create external table\n     CREATE EXTERNAL TABLE YearlySalesTotals\n     WITH (\n             LOCATION = 'sales/yearlysalesFromProc/',\n             DATA_SOURCE = sales_data,\n             FILE_FORMAT = ParquetFormat\n         )\n     AS\n     SELECT YEAR(OrderDate) AS CalendarYear,\n             SUM(Quantity) AS ItemsSold,\n             ROUND(SUM(UnitPrice) - SUM(TaxAmount), 2) AS NetRevenue\n     FROM\n         OPENROWSET(\n             BULK 'sales/csv/*.csv',\n             DATA_SOURCE = 'sales_data',\n             FORMAT = 'CSV',\n             PARSER_VERSION = '2.0',\n             HEADER_ROW = TRUE\n         ) AS orders\n     GROUP BY YEAR(OrderDate)\n END",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/DeltaTablesExplore')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "sparksmgxwkq",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "0039a76d-1d77-4b8f-bba7-cec7924b6651"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/4464e296-488b-4adc-8e86-bb0ce7983c14/resourceGroups/dp203-smgxwkq/providers/Microsoft.Synapse/workspaces/synapsesmgxwkq/bigDataPools/sparksmgxwkq",
						"name": "sparksmgxwkq",
						"type": "Spark",
						"endpoint": "https://synapsesmgxwkq.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/sparksmgxwkq",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							},
							"collapsed": false
						},
						"source": [
							"%%pyspark\r\n",
							"df = spark.read.load('abfss://files@datalakesmgxwkq.dfs.core.windows.net/products/products.csv', format='csv'\r\n",
							"## If header exists uncomment line below\r\n",
							", header=True\r\n",
							")\r\n",
							"display(df.limit(10))"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df.count()"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							" delta_table_path = \"/delta/products-delta\"\r\n",
							" df.write.format(\"delta\").save(delta_table_path)"
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"from delta.tables import *\r\n",
							"from pyspark.sql.functions import *\r\n",
							"\r\n",
							" # Create a deltaTable object\r\n",
							"deltaTable = DeltaTable.forPath(spark, delta_table_path)\r\n",
							"\r\n",
							" # Update the table (reduce price of product 771 by 10%)\r\n",
							"deltaTable.update(\r\n",
							"    condition = \"ProductID == 771\",\r\n",
							"    set = { \"ListPrice\": \"ListPrice * 0.9\" })\r\n",
							"\r\n",
							" # View the updated data as a dataframe\r\n",
							"deltaTable.toDF().show(10)"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# read data from the Delta folder\r\n",
							"\r\n",
							"new_df = spark.read.format(\"delta\").load(delta_table_path)\r\n",
							"new_df.show(10)"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"new_df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_table_path)\r\n",
							"new_df.show(10)"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							" deltaTable.history(10).show(20, False, True)"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Create Catalog tables\r\n",
							"\r\n",
							"spark.sql(\"CREATE DATABASE AdventureWorks\")\r\n",
							"spark.sql(\"CREATE TABLE AdventureWorks.ProductsExternal USING DELTA LOCATION '{0}'\".format(delta_table_path))\r\n",
							"spark.sql(\"DESCRIBE EXTENDED AdventureWorks.ProductsExternal\").show(truncate=False)"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							" %%sql\r\n",
							"\r\n",
							" USE AdventureWorks;\r\n",
							"\r\n",
							" SELECT * FROM ProductsExternal;"
						],
						"outputs": [],
						"execution_count": 9
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# using delta table for Streaming data\r\n",
							"\r\n",
							"from notebookutils import mssparkutils\r\n",
							"from pyspark.sql.types import *\r\n",
							"from pyspark.sql.functions import *\r\n",
							"\r\n",
							" # Create a folder\r\n",
							"inputPath = '/data/'\r\n",
							"mssparkutils.fs.mkdirs(inputPath)\r\n",
							"\r\n",
							" # Create a stream that reads data from the folder, using a JSON schema\r\n",
							"jsonSchema = StructType([\r\n",
							"StructField(\"device\", StringType(), False),\r\n",
							"StructField(\"status\", StringType(), False)\r\n",
							"])\r\n",
							"iotstream = spark.readStream.schema(jsonSchema).option(\"maxFilesPerTrigger\", 1).json(inputPath)\r\n",
							"\r\n",
							" # Write some event data to the folder\r\n",
							"device_data = '''{\"device\":\"Dev1\",\"status\":\"ok\"}\r\n",
							"{\"device\":\"Dev1\",\"status\":\"ok\"}\r\n",
							"{\"device\":\"Dev1\",\"status\":\"ok\"}\r\n",
							"{\"device\":\"Dev2\",\"status\":\"error\"}\r\n",
							"{\"device\":\"Dev1\",\"status\":\"ok\"}\r\n",
							"{\"device\":\"Dev1\",\"status\":\"error\"}\r\n",
							"{\"device\":\"Dev2\",\"status\":\"ok\"}\r\n",
							"{\"device\":\"Dev2\",\"status\":\"error\"}\r\n",
							"{\"device\":\"Dev1\",\"status\":\"ok\"}'''\r\n",
							"mssparkutils.fs.put(inputPath + \"data.txt\", device_data, True)\r\n",
							"print(\"Source stream created...\")"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							" # Write the stream to a delta table\r\n",
							"delta_stream_table_path = '/delta/iotdevicedata'\r\n",
							"checkpointpath = '/delta/checkpoint'\r\n",
							"deltastream = iotstream.writeStream.format(\"delta\").option(\"checkpointLocation\", checkpointpath).start(delta_stream_table_path)\r\n",
							"print(\"Streaming to delta sink...\")"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							" # Read the data in delta format into a dataframe\r\n",
							" df = spark.read.format(\"delta\").load(delta_stream_table_path)\r\n",
							" display(df)"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							" # create a catalog table based on the streaming sink\r\n",
							"spark.sql(\"CREATE TABLE IotDeviceData USING DELTA LOCATION '{0}'\".format(delta_stream_table_path))"
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							" %%sql\r\n",
							"\r\n",
							" SELECT * FROM IotDeviceData;"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"DESCRIBE EXTENDED IotDeviceData"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							" # Add more data to the source stream\r\n",
							" more_data = '''{\"device\":\"Dev1\",\"status\":\"ok\"}\r\n",
							" {\"device\":\"Dev1\",\"status\":\"ok\"}\r\n",
							" {\"device\":\"Dev1\",\"status\":\"ok\"}\r\n",
							" {\"device\":\"Dev1\",\"status\":\"ok\"}\r\n",
							" {\"device\":\"Dev1\",\"status\":\"error\"}\r\n",
							" {\"device\":\"Dev2\",\"status\":\"error\"}\r\n",
							" {\"device\":\"Dev1\",\"status\":\"ok\"}'''\r\n",
							"\r\n",
							" mssparkutils.fs.put(inputPath + \"more-data.txt\", more_data, True)"
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							" %%sql\r\n",
							" SELECT * FROM IotDeviceData;"
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Stopping a delta stream process\r\n",
							"\r\n",
							"deltastream.stop()"
						],
						"outputs": [],
						"execution_count": 21
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/FilesExploreInSpark_001')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkExplore",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "e78bd879-9efd-41af-ba2a-a84ca550fec5"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/756ae8d4-cae2-4704-b0a3-acf0fd7bb030/resourceGroups/dp203-8heknbv/providers/Microsoft.Synapse/workspaces/synapse8heknbv/bigDataPools/SparkExplore",
						"name": "SparkExplore",
						"type": "Spark",
						"endpoint": "https://synapse8heknbv.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkExplore",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "python"
							},
							"collapsed": false
						},
						"source": [
							"%%pyspark\r\n",
							"df = spark.read.load('abfss://files@datalake8heknbv.dfs.core.windows.net/sales/csv/*.csv', format='csv'\r\n",
							", header=True\r\n",
							")\r\n",
							"display(df.limit(10))"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df.printSchema()"
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from pyspark.sql.types import *\r\n",
							"from pyspark.sql.functions import *\r\n",
							"\r\n",
							"orderSchema = StructType([\r\n",
							"     StructField(\"SalesOrderNumber\", StringType()),\r\n",
							"     StructField(\"SalesOrderLineNumber\", IntegerType()),\r\n",
							"     StructField(\"OrderDate\", DateType()),\r\n",
							"     StructField(\"CustomerName\", StringType()),\r\n",
							"     StructField(\"Email\", StringType()),\r\n",
							"     StructField(\"Item\", StringType()),\r\n",
							"     StructField(\"Quantity\", IntegerType()),\r\n",
							"     StructField(\"UnitPrice\", FloatType()),\r\n",
							"     StructField(\"Tax\", FloatType())\r\n",
							"     ])\r\n",
							"\r\n",
							"df_Explicit_Schema = spark.read.load('abfss://files@datalake8heknbv.dfs.core.windows.net/sales/csv/*.csv', format='csv', schema=orderSchema)\r\n",
							"display(df_Explicit_Schema.limit(100))"
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_Explicit_Schema.printSchema()"
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"df_Explicit_Schema_Analysis = df_Explicit_Schema.select(\"CustomerName\", \"Email\")\r\n",
							"display(df_Explicit_Schema_Analysis)"
						],
						"outputs": [],
						"execution_count": 27
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"df_Explicit_Schema_Analysis = df_Explicit_Schema.select(\"CustomerName\", \"Email\", \"SalesOrderNumber\").groupBy(\"CustomerName\", \"Email\").agg(count(\"SalesOrderNumber\").alias(\"NoOfSalesOrder\")).orderBy(\"NoOfSalesOrder\")\r\n",
							"display(df_Explicit_Schema_Analysis)"
						],
						"outputs": [],
						"execution_count": 36
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"df_Explicit_Schema.createOrReplaceTempView(\"SalesData\")"
						],
						"outputs": [],
						"execution_count": 40
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"SELECT * FROM SalesData"
						],
						"outputs": [],
						"execution_count": 41
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							" %%sql\r\n",
							" SELECT YEAR(OrderDate) AS OrderYear,\r\n",
							"        SUM((UnitPrice * Quantity) + Tax) AS GrossRevenue\r\n",
							" FROM SalesData\r\n",
							" WHERE YEAR(OrderDate) IS NOT NULL\r\n",
							" GROUP BY YEAR(OrderDate)\r\n",
							" ORDER BY OrderYear;"
						],
						"outputs": [],
						"execution_count": 45
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							" sqlQuery = \"SELECT CAST(YEAR(OrderDate) AS CHAR(4)) AS OrderYear, \\\r\n",
							"                 SUM((UnitPrice * Quantity) + Tax) AS GrossRevenue \\\r\n",
							"             FROM SalesData \\\r\n",
							"             WHERE YEAR(OrderDate) IS NOT NULL \\\r\n",
							"             GROUP BY CAST(YEAR(OrderDate) AS CHAR(4)) \\\r\n",
							"             ORDER BY OrderYear\"\r\n",
							" df_spark = spark.sql(sqlQuery)\r\n",
							" df_spark.show()"
						],
						"outputs": [],
						"execution_count": 50
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							" from matplotlib import pyplot as plt\r\n",
							"\r\n",
							" # matplotlib requires a Pandas dataframe, not a Spark one\r\n",
							" df_sales = df_spark.toPandas()\r\n",
							"\r\n",
							" # Create a bar plot of revenue by year\r\n",
							" plt.bar(x=df_sales['OrderYear'], height=df_sales['GrossRevenue'])\r\n",
							"\r\n",
							" # Display the plot\r\n",
							" plt.show()"
						],
						"outputs": [],
						"execution_count": 51
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							" # Clear the plot area\r\n",
							" plt.clf()\r\n",
							"\r\n",
							" # Create a bar plot of revenue by year\r\n",
							" plt.bar(x=df_sales['OrderYear'], height=df_sales['GrossRevenue'], color='orange')\r\n",
							"\r\n",
							" # Customize the chart\r\n",
							" plt.title('Revenue by Year')\r\n",
							" plt.xlabel('Year')\r\n",
							" plt.ylabel('Revenue')\r\n",
							" plt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y', alpha=0.7)\r\n",
							" plt.xticks(rotation=45)\r\n",
							"\r\n",
							" # Show the figure\r\n",
							" plt.show()"
						],
						"outputs": [],
						"execution_count": 52
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							" # Clear the plot area\r\n",
							" plt.clf()\r\n",
							"\r\n",
							" # Create a Figure\r\n",
							" fig = plt.figure(figsize=(8,3))\r\n",
							"\r\n",
							" # Create a bar plot of revenue by year\r\n",
							" plt.bar(x=df_sales['OrderYear'], height=df_sales['GrossRevenue'], color='orange')\r\n",
							"\r\n",
							" # Customize the chart\r\n",
							" plt.title('Revenue by Year')\r\n",
							" plt.xlabel('Year')\r\n",
							" plt.ylabel('Revenue')\r\n",
							" plt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y', alpha=0.7)\r\n",
							" plt.xticks(rotation=45)\r\n",
							"\r\n",
							" # Show the figure\r\n",
							" plt.show()"
						],
						"outputs": [],
						"execution_count": 53
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							" # Clear the plot area\r\n",
							" plt.clf()\r\n",
							"\r\n",
							" # Create a figure for 2 subplots (1 row, 2 columns)\r\n",
							" fig, ax = plt.subplots(1, 2, figsize = (10,4))\r\n",
							"\r\n",
							" # Create a bar plot of revenue by year on the first axis\r\n",
							" ax[0].bar(x=df_sales['OrderYear'], height=df_sales['GrossRevenue'], color='orange')\r\n",
							" ax[0].set_title('Revenue by Year')\r\n",
							"\r\n",
							" # Create a pie chart of yearly order counts on the second axis\r\n",
							" yearly_counts = df_sales['OrderYear'].value_counts()\r\n",
							" ax[1].pie(yearly_counts)\r\n",
							" ax[1].set_title('Orders per Year')\r\n",
							" ax[1].legend(yearly_counts.keys().tolist())\r\n",
							"\r\n",
							" # Add a title to the Figure\r\n",
							" fig.suptitle('Sales Data')\r\n",
							"\r\n",
							" # Show the figure\r\n",
							" plt.show()"
						],
						"outputs": [],
						"execution_count": 54
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							" import seaborn as sns\r\n",
							"\r\n",
							" # Clear the plot area\r\n",
							" plt.clf()\r\n",
							"\r\n",
							" # Create a bar chart\r\n",
							" ax = sns.barplot(x=\"OrderYear\", y=\"GrossRevenue\", data=df_sales)\r\n",
							" plt.show()"
						],
						"outputs": [],
						"execution_count": 55
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							" # Clear the plot area\r\n",
							" plt.clf()\r\n",
							"\r\n",
							" # Set the visual theme for seaborn\r\n",
							" sns.set_theme(style=\"whitegrid\")\r\n",
							"\r\n",
							" # Create a bar chart\r\n",
							" ax = sns.barplot(x=\"OrderYear\", y=\"GrossRevenue\", data=df_sales)\r\n",
							" plt.show()"
						],
						"outputs": [],
						"execution_count": 56
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Clear the plot area\r\n",
							"plt.clf()\r\n",
							"\r\n",
							" # Create a bar chart\r\n",
							"ax = sns.lineplot(x=\"OrderYear\", y=\"GrossRevenue\", data=df_sales)\r\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 58
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							""
						],
						"outputs": [],
						"execution_count": null
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Spark Transform')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SparkExplore",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "130792cd-b25a-45a4-ade5-244c57bf4a95"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/756ae8d4-cae2-4704-b0a3-acf0fd7bb030/resourceGroups/dp203-8heknbv/providers/Microsoft.Synapse/workspaces/synapse8heknbv/bigDataPools/SparkExplore",
						"name": "SparkExplore",
						"type": "Spark",
						"endpoint": "https://synapse8heknbv.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SparkExplore",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 10,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"source": [
							"# Transform data by using Spark\n",
							"\n",
							"Apache Spark provides a distributed data processing platform that you can use to perform complex data transformations at scale.\n",
							"\n",
							"\n",
							"## Load source data\n",
							"\n",
							"Let's start by loading some historical sales order data into a dataframe.\n",
							"\n",
							"Review the code in the cell below, which loads the sales order from all of the csv files within the **data** directory. Then click the **&#9655;** button to the left of the cell to run it.\n",
							"\n",
							"> **Note**: The first time you run a cell in a notebook, the Spark pool must be started; which can take several minutes."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"order_details = spark.read.csv('/sales/csv/*.csv', header=True, inferSchema=True)\n",
							"display(order_details.limit(5))"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Transform the data structure\r\n",
							"\r\n",
							"The source data includes a **CustomerName** field, that contains the customer's first and last name. Let's modify the dataframe to separate this field into separate **FirstName** and **LastName** fields."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from pyspark.sql.functions import split, col\r\n",
							"\r\n",
							"# Create the new FirstName and LastName fields\r\n",
							"transformed_df = order_details.withColumn(\"FirstName\", split(col(\"CustomerName\"), \" \").getItem(0)).withColumn(\"LastName\", split(col(\"CustomerName\"), \" \").getItem(1))\r\n",
							"\r\n",
							"# Remove the CustomerName field\r\n",
							"transformed_df = transformed_df.drop(\"CustomerName\")\r\n",
							"\r\n",
							"display(transformed_df.limit(5))"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"The code above creates a new dataframe with the **CustomerName** field removed and two new **FirstName** and **LastName** fields.\r\n",
							"\r\n",
							"You can use the full power of the Spark SQL library to transform the data by filtering rows, deriving, removing, renaming columns, and any applying other required data modifications.\r\n",
							"\r\n",
							"## Save the transformed data\r\n",
							"\r\n",
							"After making the required changes to the data, you can save the results in a supported file format.\r\n",
							"\r\n",
							"> **Note**: Commonly, *Parquet* format is preferred for data files that you will use for further analysis or ingestion into an analytical store. Parquet is a very efficient format that is supported by most large scale data analytics systems. In fact, sometimes your data transformation requirement may simply be to convert data from another format (such as CSV) to Parquet!\r\n",
							"\r\n",
							"Use the following code to save the transformed dataframe in Parquet format (Overwriting the data if it already exists)."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"transformed_df.write.mode(\"overwrite\").parquet('/transformed_data/orders.parquet')\r\n",
							"print (\"Transformed data saved!\")"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"In the **files** tab (which should still be open above), navigate to the root **files** container and verify that a new folder named **transformed_data** has been created, containing a file named **orders.parquet**. Then return to this notebook."
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"## Partition data\n",
							"\n",
							"A common way to optimize performance when dealing with large volumes of data is to partition the data files based on one or more field values. This can significant improve performance and make it easier to filter data.\n",
							"\n",
							"Use the following cell to derive new **Year** and **Month** fields and then save the resulting data in Parquet format, partitioned by year and month."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"from pyspark.sql.functions import year, month, col\r\n",
							"\r\n",
							"dated_df = transformed_df.withColumn(\"Year\", year(col(\"OrderDate\"))).withColumn(\"Month\", month(col(\"OrderDate\")))\r\n",
							"display(dated_df.limit(5))\r\n",
							"dated_df.write.partitionBy(\"Year\",\"Month\").mode(\"overwrite\").parquet(\"/partitioned_data\")\r\n",
							"print (\"Transformed data saved!\")"
						],
						"outputs": [],
						"execution_count": 8
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"In the **files** tab (which should still be open above), navigate to the root **files** container and verify that a new folder named **partitioned_data** has been created, containing a hierachy of folders in the format **Year=*NNNN*** / **Month=*N***, each containing a .parquet file for the orders placed in the corresponding year and month. Then return to this notebook.\r\n",
							"\r\n",
							"You can read this data into a dataframe from any folder in the hierarchy, using explicit values or wildcards for partitioning fields. For example, use the following code to get the sales orders placed in 2020 for all months."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"orders_2020 = spark.read.parquet('/partitioned_data/Year=2020/Month=*')\r\n",
							"display(orders_2020.limit(5))"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Note that the partitioning columns specified in the file path are omitted in the resulting dataframe.\r\n",
							"\r\n",
							"## Use SQL to transform data\r\n",
							"\r\n",
							"Spark is a very flexible platform, and the **SQL** library that provides the dataframe also enables you to work with data using SQL semantics. You can query and transform data in dataframes by using SQL queries, and persist the results as tables - which are metadata abstractions over files.\r\n",
							"\r\n",
							"First, use the following code to save the original sales orders data (loaded from CSV files) as a table. Technically, this is an *external* table because the **path** parameter is used to specify where the data files for the table are stored (an *internal* table is stored in the system storage for the Spark metastore and managed automatically)."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": false,
								"source_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"order_details.write.saveAsTable('sales_orders', format='parquet', mode='overwrite', path='/sales_orders_table')"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"In the **files** tab (which should still be open above), navigate to the root **files** container and verify that a new folder named **sales_orders_table** has been created, containing parquet files for the table data. Then return to this notebook.\r\n",
							"\r\n",
							"Now that the table has been created, you can use SQL to transform it. For example, the following code derives new Year and Month columns and then saves the results as a partitioned external table."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"sql_transform = spark.sql(\"SELECT *, YEAR(OrderDate) AS Year, MONTH(OrderDate) AS Month FROM sales_orders\")\r\n",
							"display(sql_transform.limit(5))\r\n",
							"sql_transform.write.partitionBy(\"Year\",\"Month\").saveAsTable('transformed_orders', format='parquet', mode='overwrite', path='/transformed_orders_table')"
						],
						"outputs": [],
						"execution_count": 14
					},
					{
						"cell_type": "markdown",
						"source": [
							"In the **files** tab (which should still be open above), navigate to the root **files** container and verify that a new folder named **transformed_orders_table** has been created, containing a hierachy of folders in the format **Year=*NNNN*** / **Month=*N***, each containing a .parquet file for the orders placed in the corresponding year and month. Then return to this notebook.\n",
							"\n",
							"Essentially you've performed the same data transformation into partitioned parquet files as s before, but by using SQL instead of native dataframe methods.\n",
							"\n",
							"You can read this data into a dataframe from any folder in the hierarchy as before, but because the data files are also abstracted by a table in the metastore, you can query the data directly using SQL."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"SELECT * FROM transformed_orders\r\n",
							"WHERE Year = 2021\r\n",
							"    AND Month = 1"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Because these are *external* tables, you can drop the tables from the metastore without deleting the files - so the transfomed data remains available for other downstream data analytics or ingestion processes."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"microsoft": {
								"language": "sparksql"
							},
							"collapsed": false
						},
						"source": [
							"%%sql\r\n",
							"\r\n",
							"DROP TABLE transformed_orders;\r\n",
							"DROP TABLE sales_orders;"
						],
						"outputs": [],
						"execution_count": 16
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/RetailDB')]",
			"type": "Microsoft.Synapse/workspaces/databases",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"Ddls": [
					{
						"ActionType": "CREATE",
						"OldEntity": null,
						"NewEntity": {
							"Name": "RetailDB",
							"EntityType": "DATABASE",
							"Origin": {
								"Type": "SPARK"
							},
							"Properties": {
								"IsSyMSCDMDatabase": true,
								"DerivedModelDBInfo": "{\"ModelDirectives\":{\"BaseModel\":{\"Name\":\"Retail\",\"Version\":\"1.3.0\"}}}"
							},
							"Source": {
								"Provider": "ADLS",
								"Location": "abfss://files@datalake8heknbv.dfs.core.windows.net/RetailDB",
								"Properties": {
									"FormatType": "csv",
									"LinkedServiceName": "synapse8heknbv-WorkspaceDefaultStorage"
								}
							}
						},
						"Source": {
							"Type": "SPARK"
						}
					},
					{
						"ActionType": "CREATE",
						"OldEntity": null,
						"NewEntity": {
							"Name": "Customer",
							"EntityType": "TABLE",
							"Namespace": {
								"DatabaseName": "RetailDB"
							},
							"Description": "",
							"TableType": "EXTERNAL",
							"Origin": {
								"Type": "SPARK"
							},
							"StorageDescriptor": {
								"Columns": [
									{
										"Name": "CustomerId",
										"Description": "Unique customer ID",
										"OriginDataTypeName": {
											"TypeName": "long",
											"IsComplexType": false,
											"IsNullable": false,
											"Properties": {
												"HIVE_TYPE_STRING": "long"
											}
										}
									},
									{
										"Name": "FirstName",
										"Description": "Customer first name",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": false,
											"Length": 256,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "LastName",
										"Description": "CustomerLastName",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": false,
											"Length": 256,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "EmailAddress",
										"Description": "Customer Email",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": false,
											"Length": 256,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									},
									{
										"Name": "Phone",
										"Description": "Customer Phone Number",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": false,
											"Length": 256,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										}
									}
								],
								"Format": {
									"InputFormat": "org.apache.hadoop.mapred.SequenceFileInputFormat",
									"OutputFormat": "org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat",
									"FormatType": "csv",
									"SerializeLib": "org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe",
									"Properties": {
										"path": "abfss://files@datalake8heknbv.dfs.core.windows.net/RetailDB/Customer",
										"delimiter": ",",
										"firstRowAsHeader": "false",
										"multiLine": "false",
										"serialization.format": "1",
										"escape": "\\",
										"quote": "\"",
										"FormatTypeSetToDatabaseDefault": true,
										"header": "false"
									}
								},
								"Source": {
									"Provider": "ADLS",
									"Location": "abfss://files@datalake8heknbv.dfs.core.windows.net/RetailDB/Customer",
									"Properties": {
										"LinkedServiceName": "synapse8heknbv-WorkspaceDefaultStorage",
										"LocationSetToDatabaseDefault": true
									}
								},
								"Properties": {
									"textinputformat.record.delimiter": ",",
									"compression": "{\"type\":\"None\",\"level\":\"optimal\"}",
									"derivedModelAttributeInfo": "{\"attributeReferences\":{}}"
								},
								"Compressed": false,
								"IsStoredAsSubdirectories": false
							},
							"Properties": {
								"Description": "",
								"DisplayFolderInfo": "{\"name\":\"Others\",\"colorCode\":\"\"}",
								"PrimaryKeys": "CustomerId",
								"spark.sql.sources.provider": "csv"
							},
							"Retention": 0,
							"Temporary": false,
							"IsRewriteEnabled": false
						},
						"Source": {
							"Type": "SPARK"
						}
					},
					{
						"ActionType": "CREATE",
						"OldEntity": null,
						"NewEntity": {
							"Name": "Product",
							"EntityType": "TABLE",
							"Namespace": {
								"DatabaseName": "RetailDB"
							},
							"Description": "A product is anything that can be offered to a market that might satisfy a want or need by potential customers.    That product is the sum of all physical, psychological, symbolic, and service attributes associated with it.\n\nThere are two basic types of products:\n\n- Tangible (physical)\n- Intangible (non-physical) such as services\n\nA service is a non-material or intangible product - such as professional consultancy, maintenance service, repair service etc.\nEach product has its own benefits, application, brand name, and packaging that gives it its own identity and distinguishing characteristics.\n\nEvery business or organization has business rules that define precisely what a product is.    While we intuitively know what a product is, we must quantify that knowledge and associated business rules with consistent definitions that can be implemented within the organization in strategies and applications.\n\nA product typically goes through five stages of development:\n\n(1) Idea Stage - involving a thorough evaluation of the potential product\n\n(2) Concept Stage - determines customer acceptance by testing and presentation to consumers and distribution channel members.   Specific aspects regarding quality, dependability, reliability, warranty, packaging, service, pricing, terms of sale, sales and distribution channels, advertising and promotions are evaluated.\n\n(3) Product Development Stage - transforms the prototype product into an actual product for mass sale.   This stage requires close interaction between both marketing and manufacturing.\n\n(4) Test Marketing Stage - may or may not be used since it is an expensive and time-consuming process.  Test marketing involves evaluating various product options and alternatives.\n\n(5) Commercialization - It is very expensive to launch a new product so commercialization only applies to those specific products that are actually going to be sold to the market.\n\nProducts tend to be categorized as either:  Industrial goods and consumer goods\n\nIndustrial goods are used to produce other products .\n\nIndustrial goods may be further divided into:\n\n- Raw materials\n- Equipment\n- Pre-built materials \n- Supplies.\n\nConsumer goods are intended for consumption by the general public.\n\nConsumer goods may be further divided into:\n\n- Durable goods\n- Nondurable goods\n- Packaged goods\n\nA product may be a member of a product family or product line.\n\nA product family is a grouping of products or services that are related to each other by common function, functionality, design platform or similar characteristics.\n\nMembers of a product family frequently have many common parts and assemblies.\n\nProduct families are the highest level of grouping for forecasting, capacity planning or related functions.\n\nEx:\nThe Apple Macintosh family of products consists of the product lines:\n- Mac mini\n- MacBook Pro\n- Mac Pro\n\nA product line is a grouping of products that are closely related in usage, functionality or marketing characteristics.\n\nA Product Family typically is created to address one or five functions:\n\n1. To increase profits and not erode the sales of existing products\n\n2. To attract additional Markets or Market Segments\n\n3. To counter competitor's products\n\n4. To fill a gap in an existing Product Family.\n\n5. To promote sales of other products in the family line\n\nLine Depth refers to the number of products in the product line.\n\nLine consistency refers to how closely related the products are that make up the product line.\n\nLine vulnerability refers to the percentage of sales or profits that are derived from only a few products in the product line.\n\nProduct width refers to the number of different product lines sold by a company.\n\nProduct mix refers to the total number of products sold in all product lines.\n\nLine extension refers to the adding of a new product to a line.\n\n\"Trading up or brand leveraging\" refers to adding a product of better quality to a product line than the other products in that line.\n\n\"Trading down\" refers to adding a product of lesser quality to a product line than the other products in that line.\n\nIf a line of products is sold with the same brand name, this is referred to as family branding.\nStrategy and decisions regarding a product line are usually incorporated in a high-level marketing plan addressing product line strategy, sales, channels, distribution channels, pricing and related issues.\nA product-line manager is responsible for a product line and supervises several product managers who are responsible for individual products within the line.\nProduct-line managers typically have the following responsibilities:\n- Expansion and composition of a product line\n- Evaluate the effects of product mixes on the profitability of other items in the line\n- Planning and allocation of resources to individual products in the line\nA product is normally associated with a brand strategy - manufacturer, private or generic.\n\n1. Manufacturer-  or 'national' branding in which the brand is assigned by the manufacturer of the Product.\n\n2. Private - or 'dealer' branding in which the brand is assigned by the retailer or wholesaler of the Product.\n\n3. Generic - in which the Product is not marked with any identification.   Generic brands are a means for manufacturers to increase profits by saving on advertising, packaging or other costs associated with manufacturer or private branding.\n\nA brand is name, term, sign, symbol or design or a combination of these which identify the goods or services and differentiate them from those of competitors'\n\nA Trade mark is a brand or some part of the brand that is given legal protection because it is capable of exclusive appropriation and representation.\n\nManufacturers can use their own brands (known as Manufacturers' brands) or brands of their distributors (Distributors' brands).\n\nManufacturers/ distributors use brand names for a variety of reasons ranging from simple identification purposes to having legal protection for unique features of the products from imitations.\n\nBrands help consumers recognize certain quality parameters. In some cases, brands are just used to endow the product with unique story and character which itself can be a basis for product differentiation.\n\nIndividual brands have their own identity and the corporate or common name is not used to promote its equity.\n\nIndividual branding requires more expensive advertising and brand extensive brand creation investments.  By extension, each new brand does not benefit from the positive perceptions of earlier brands.\n\nBy contrast, family branding has several advantages.\n\nEach new product is quickly associated with the other products and brand in terms of quality and benefits.\n\nReduced or eliminated time for name identification and advertising for name recognition purposes.",
							"TableType": "EXTERNAL",
							"Origin": {
								"Type": "SPARK"
							},
							"StorageDescriptor": {
								"Columns": [
									{
										"Name": "ProductId",
										"Description": "The unique identifier of a Product.",
										"OriginDataTypeName": {
											"TypeName": "long",
											"IsComplexType": false,
											"IsNullable": false,
											"Properties": {
												"HIVE_TYPE_STRING": "long"
											}
										},
										"BaseAttributeReference": {
											"Entity": "RetailProduct.cdm.json/RetailProduct",
											"Name": "ProductId"
										}
									},
									{
										"Name": "ProductName",
										"Description": "The name of the Product, which normally corresponds to the 'marketing name' of the Product.",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 256,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										},
										"BaseAttributeReference": {
											"Entity": "RetailProduct.cdm.json/RetailProduct",
											"Name": "ProductName"
										}
									},
									{
										"Name": "IntroductionDate",
										"Description": "The date that the Product was introduced for sale.",
										"OriginDataTypeName": {
											"TypeName": "date",
											"IsComplexType": false,
											"IsNullable": true,
											"Properties": {
												"DateFormat": "YYYY-MM-DD",
												"HIVE_TYPE_STRING": "date"
											}
										},
										"BaseAttributeReference": {
											"Entity": "RetailProduct.cdm.json/RetailProduct",
											"Name": "IntroductionDate"
										}
									},
									{
										"Name": "ActualAbandonmentDate",
										"Description": "The actual date that the marketing of the product was discontinued. \n\nAbandonment is a component in the decline stage of the product's life cycle characterized by a reduced market demand for the product and an increased number of competing products with similar characteristics.\n\nThere are three (3) strategies for abandoning a product:\n\n(1)  Reduced marketing and expenditures to maintain profits.\n\n(2)  Concentrating on the strongest market segments and eliminating the weaker market segments\n\n(3)  Maintain the marketing level until the product is discontinued.",
										"OriginDataTypeName": {
											"TypeName": "date",
											"IsComplexType": false,
											"IsNullable": true,
											"Properties": {
												"DateFormat": "YYYY-MM-DD",
												"HIVE_TYPE_STRING": "date"
											}
										},
										"BaseAttributeReference": {
											"Entity": "RetailProduct.cdm.json/RetailProduct",
											"Name": "ActualAbandonmentDate"
										}
									},
									{
										"Name": "ProductGrossWeight",
										"Description": "The gross product weight.",
										"OriginDataTypeName": {
											"TypeName": "decimal",
											"IsComplexType": false,
											"IsNullable": true,
											"Precision": 18,
											"Scale": 8,
											"Properties": {
												"HIVE_TYPE_STRING": "decimal"
											}
										},
										"BaseAttributeReference": {
											"Entity": "RetailProduct.cdm.json/RetailProduct",
											"Name": "ProductGrossWeight"
										}
									},
									{
										"Name": "ItemSku",
										"Description": "The Stock Keeping Unit identifier, which is typically used for inventory-related activities.",
										"OriginDataTypeName": {
											"TypeName": "string",
											"IsComplexType": false,
											"IsNullable": true,
											"Length": 40,
											"Properties": {
												"HIVE_TYPE_STRING": "string"
											}
										},
										"BaseAttributeReference": {
											"Entity": "RetailProduct.cdm.json/RetailProduct",
											"Name": "ItemSku"
										}
									},
									{
										"Name": "ListPrice",
										"Description": "The Product Price",
										"OriginDataTypeName": {
											"TypeName": "decimal",
											"IsComplexType": false,
											"IsNullable": false,
											"Precision": 18,
											"Scale": 2,
											"Properties": {
												"HIVE_TYPE_STRING": "decimal"
											}
										}
									}
								],
								"Format": {
									"InputFormat": "org.apache.hadoop.mapred.SequenceFileInputFormat",
									"OutputFormat": "org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat",
									"FormatType": "csv",
									"SerializeLib": "org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe",
									"Properties": {
										"path": "abfss://files@datalake8heknbv.dfs.core.windows.net/RetailDB/Product",
										"delimiter": ",",
										"firstRowAsHeader": "false",
										"multiLine": "false",
										"serialization.format": "1",
										"escape": "\\",
										"quote": "\"",
										"FormatTypeSetToDatabaseDefault": true,
										"header": "false"
									}
								},
								"Source": {
									"Provider": "ADLS",
									"Location": "abfss://files@datalake8heknbv.dfs.core.windows.net/RetailDB/Product",
									"Properties": {
										"LinkedServiceName": "synapse8heknbv-WorkspaceDefaultStorage",
										"LocationSetToDatabaseDefault": true
									}
								},
								"Properties": {
									"textinputformat.record.delimiter": ",",
									"compression": "{\"type\":\"None\",\"level\":\"optimal\"}",
									"derivedModelAttributeInfo": "{\"attributeReferences\":{\"ProductId\":{\"entity\":\"RetailProduct.cdm.json/RetailProduct\",\"name\":\"ProductId\"},\"ProductName\":{\"entity\":\"RetailProduct.cdm.json/RetailProduct\",\"name\":\"ProductName\"},\"IntroductionDate\":{\"entity\":\"RetailProduct.cdm.json/RetailProduct\",\"name\":\"IntroductionDate\"},\"ActualAbandonmentDate\":{\"entity\":\"RetailProduct.cdm.json/RetailProduct\",\"name\":\"ActualAbandonmentDate\"},\"ProductGrossWeight\":{\"entity\":\"RetailProduct.cdm.json/RetailProduct\",\"name\":\"ProductGrossWeight\"},\"ItemSku\":{\"entity\":\"RetailProduct.cdm.json/RetailProduct\",\"name\":\"ItemSku\"}}}"
								},
								"Compressed": false,
								"IsStoredAsSubdirectories": false
							},
							"Properties": {
								"DerivedModelEntityInfo": "{\"entityDirectives\":{\"name\":\"Product\",\"description\":\"A product is anything that can be offered to a market that might satisfy a want or need by potential customers.    That product is the sum of all physical, psychological, symbolic, and service attributes associated with it.\\n\\nThere are two basic types of products:\\n\\n- Tangible (physical)\\n- Intangible (non-physical) such as services\\n\\nA service is a non-material or intangible product - such as professional consultancy, maintenance service, repair service etc.\\nEach product has its own benefits, application, brand name, and packaging that gives it its own identity and distinguishing characteristics.\\n\\nEvery business or organization has business rules that define precisely what a product is.    While we intuitively know what a product is, we must quantify that knowledge and associated business rules with consistent definitions that can be implemented within the organization in strategies and applications.\\n\\nA product typically goes through five stages of development:\\n\\n(1) Idea Stage - involving a thorough evaluation of the potential product\\n\\n(2) Concept Stage - determines customer acceptance by testing and presentation to consumers and distribution channel members.   Specific aspects regarding quality, dependability, reliability, warranty, packaging, service, pricing, terms of sale, sales and distribution channels, advertising and promotions are evaluated.\\n\\n(3) Product Development Stage - transforms the prototype product into an actual product for mass sale.   This stage requires close interaction between both marketing and manufacturing.\\n\\n(4) Test Marketing Stage - may or may not be used since it is an expensive and time-consuming process.  Test marketing involves evaluating various product options and alternatives.\\n\\n(5) Commercialization - It is very expensive to launch a new product so commercialization only applies to those specific products that are actually going to be sold to the market.\\n\\nProducts tend to be categorized as either:  Industrial goods and consumer goods\\n\\nIndustrial goods are used to produce other products .\\n\\nIndustrial goods may be further divided into:\\n\\n- Raw materials\\n- Equipment\\n- Pre-built materials \\n- Supplies.\\n\\nConsumer goods are intended for consumption by the general public.\\n\\nConsumer goods may be further divided into:\\n\\n- Durable goods\\n- Nondurable goods\\n- Packaged goods\\n\\nA product may be a member of a product family or product line.\\n\\nA product family is a grouping of products or services that are related to each other by common function, functionality, design platform or similar characteristics.\\n\\nMembers of a product family frequently have many common parts and assemblies.\\n\\nProduct families are the highest level of grouping for forecasting, capacity planning or related functions.\\n\\nEx:\\nThe Apple Macintosh family of products consists of the product lines:\\n- Mac mini\\n- MacBook Pro\\n- Mac Pro\\n\\nA product line is a grouping of products that are closely related in usage, functionality or marketing characteristics.\\n\\nA Product Family typically is created to address one or five functions:\\n\\n1. To increase profits and not erode the sales of existing products\\n\\n2. To attract additional Markets or Market Segments\\n\\n3. To counter competitor's products\\n\\n4. To fill a gap in an existing Product Family.\\n\\n5. To promote sales of other products in the family line\\n\\nLine Depth refers to the number of products in the product line.\\n\\nLine consistency refers to how closely related the products are that make up the product line.\\n\\nLine vulnerability refers to the percentage of sales or profits that are derived from only a few products in the product line.\\n\\nProduct width refers to the number of different product lines sold by a company.\\n\\nProduct mix refers to the total number of products sold in all product lines.\\n\\nLine extension refers to the adding of a new product to a line.\\n\\n\\\"Trading up or brand leveraging\\\" refers to adding a product of better quality to a product line than the other products in that line.\\n\\n\\\"Trading down\\\" refers to adding a product of lesser quality to a product line than the other products in that line.\\n\\nIf a line of products is sold with the same brand name, this is referred to as family branding.\\nStrategy and decisions regarding a product line are usually incorporated in a high-level marketing plan addressing product line strategy, sales, channels, distribution channels, pricing and related issues.\\nA product-line manager is responsible for a product line and supervises several product managers who are responsible for individual products within the line.\\nProduct-line managers typically have the following responsibilities:\\n- Expansion and composition of a product line\\n- Evaluate the effects of product mixes on the profitability of other items in the line\\n- Planning and allocation of resources to individual products in the line\\nA product is normally associated with a brand strategy - manufacturer, private or generic.\\n\\n1. Manufacturer-  or 'national' branding in which the brand is assigned by the manufacturer of the Product.\\n\\n2. Private - or 'dealer' branding in which the brand is assigned by the retailer or wholesaler of the Product.\\n\\n3. Generic - in which the Product is not marked with any identification.   Generic brands are a means for manufacturers to increase profits by saving on advertising, packaging or other costs associated with manufacturer or private branding.\\n\\nA brand is name, term, sign, symbol or design or a combination of these which identify the goods or services and differentiate them from those of competitors'\\n\\nA Trade mark is a brand or some part of the brand that is given legal protection because it is capable of exclusive appropriation and representation.\\n\\nManufacturers can use their own brands (known as Manufacturers' brands) or brands of their distributors (Distributors' brands).\\n\\nManufacturers/ distributors use brand names for a variety of reasons ranging from simple identification purposes to having legal protection for unique features of the products from imitations.\\n\\nBrands help consumers recognize certain quality parameters. In some cases, brands are just used to endow the product with unique story and character which itself can be a basis for product differentiation.\\n\\nIndividual brands have their own identity and the corporate or common name is not used to promote its equity.\\n\\nIndividual branding requires more expensive advertising and brand extensive brand creation investments.  By extension, each new brand does not benefit from the positive perceptions of earlier brands.\\n\\nBy contrast, family branding has several advantages.\\n\\nEach new product is quickly associated with the other products and brand in terms of quality and benefits.\\n\\nReduced or eliminated time for name identification and advertising for name recognition purposes.\",\"baseEntityReference\":{\"name\":\"RetailProduct\",\"path\":\"RetailProduct.cdm.json/RetailProduct\"},\"primaryKey\":[\"ProductId\"],\"projectionInfo\":{\"attributes\":[{\"type\":\"Existing\",\"attributeReference\":{\"entity\":\"RetailProduct.cdm.json/RetailProduct\",\"name\":\"ProductId\"},\"dataType\":\"long\",\"description\":\"The unique identifier of a Product.\",\"isNullable\":false,\"name\":\"ProductId\"},{\"type\":\"Existing\",\"attributeReference\":{\"entity\":\"RetailProduct.cdm.json/RetailProduct\",\"name\":\"ProductName\"},\"dataType\":\"string\",\"dataTypeLength\":256,\"description\":\"The name of the Product, which normally corresponds to the 'marketing name' of the Product.\",\"isNullable\":true,\"name\":\"ProductName\"},{\"type\":\"Existing\",\"attributeReference\":{\"entity\":\"RetailProduct.cdm.json/RetailProduct\",\"name\":\"IntroductionDate\"},\"dataType\":\"date\",\"dateFormat\":\"YYYY-MM-DD\",\"description\":\"The date that the Product was introduced for sale.\",\"isNullable\":true,\"name\":\"IntroductionDate\"},{\"type\":\"Existing\",\"attributeReference\":{\"entity\":\"RetailProduct.cdm.json/RetailProduct\",\"name\":\"ActualAbandonmentDate\"},\"dataType\":\"date\",\"dateFormat\":\"YYYY-MM-DD\",\"description\":\"The actual date that the marketing of the product was discontinued. \\n\\nAbandonment is a component in the decline stage of the product's life cycle characterized by a reduced market demand for the product and an increased number of competing products with similar characteristics.\\n\\nThere are three (3) strategies for abandoning a product:\\n\\n(1)  Reduced marketing and expenditures to maintain profits.\\n\\n(2)  Concentrating on the strongest market segments and eliminating the weaker market segments\\n\\n(3)  Maintain the marketing level until the product is discontinued.\",\"isNullable\":true,\"name\":\"ActualAbandonmentDate\"},{\"type\":\"Existing\",\"attributeReference\":{\"entity\":\"RetailProduct.cdm.json/RetailProduct\",\"name\":\"ProductGrossWeight\"},\"dataType\":\"decimal\",\"dataTypeLength\":18,\"description\":\"The gross product weight.\",\"isNullable\":true,\"scale\":8,\"name\":\"ProductGrossWeight\"},{\"type\":\"Existing\",\"attributeReference\":{\"entity\":\"RetailProduct.cdm.json/RetailProduct\",\"name\":\"ItemSku\"},\"dataType\":\"string\",\"dataTypeLength\":40,\"description\":\"The Stock Keeping Unit identifier, which is typically used for inventory-related activities.\",\"isNullable\":true,\"name\":\"ItemSku\"},{\"type\":\"New\",\"dataType\":\"decimal\",\"dataTypeLength\":18,\"description\":\"The Product Price\",\"isNullable\":false,\"scale\":2,\"name\":\"ListPrice\"}]}}}",
								"Description": "A product is anything that can be offered to a market that might satisfy a want or need by potential customers.    That product is the sum of all physical, psychological, symbolic, and service attributes associated with it.\n\nThere are two basic types of products:\n\n- Tangible (physical)\n- Intangible (non-physical) such as services\n\nA service is a non-material or intangible product - such as professional consultancy, maintenance service, repair service etc.\nEach product has its own benefits, application, brand name, and packaging that gives it its own identity and distinguishing characteristics.\n\nEvery business or organization has business rules that define precisely what a product is.    While we intuitively know what a product is, we must quantify that knowledge and associated business rules with consistent definitions that can be implemented within the organization in strategies and applications.\n\nA product typically goes through five stages of development:\n\n(1) Idea Stage - involving a thorough evaluation of the potential product\n\n(2) Concept Stage - determines customer acceptance by testing and presentation to consumers and distribution channel members.   Specific aspects regarding quality, dependability, reliability, warranty, packaging, service, pricing, terms of sale, sales and distribution channels, advertising and promotions are evaluated.\n\n(3) Product Development Stage - transforms the prototype product into an actual product for mass sale.   This stage requires close interaction between both marketing and manufacturing.\n\n(4) Test Marketing Stage - may or may not be used since it is an expensive and time-consuming process.  Test marketing involves evaluating various product options and alternatives.\n\n(5) Commercialization - It is very expensive to launch a new product so commercialization only applies to those specific products that are actually going to be sold to the market.\n\nProducts tend to be categorized as either:  Industrial goods and consumer goods\n\nIndustrial goods are used to produce other products .\n\nIndustrial goods may be further divided into:\n\n- Raw materials\n- Equipment\n- Pre-built materials \n- Supplies.\n\nConsumer goods are intended for consumption by the general public.\n\nConsumer goods may be further divided into:\n\n- Durable goods\n- Nondurable goods\n- Packaged goods\n\nA product may be a member of a product family or product line.\n\nA product family is a grouping of products or services that are related to each other by common function, functionality, design platform or similar characteristics.\n\nMembers of a product family frequently have many common parts and assemblies.\n\nProduct families are the highest level of grouping for forecasting, capacity planning or related functions.\n\nEx:\nThe Apple Macintosh family of products consists of the product lines:\n- Mac mini\n- MacBook Pro\n- Mac Pro\n\nA product line is a grouping of products that are closely related in usage, functionality or marketing characteristics.\n\nA Product Family typically is created to address one or five functions:\n\n1. To increase profits and not erode the sales of existing products\n\n2. To attract additional Markets or Market Segments\n\n3. To counter competitor's products\n\n4. To fill a gap in an existing Product Family.\n\n5. To promote sales of other products in the family line\n\nLine Depth refers to the number of products in the product line.\n\nLine consistency refers to how closely related the products are that make up the product line.\n\nLine vulnerability refers to the percentage of sales or profits that are derived from only a few products in the product line.\n\nProduct width refers to the number of different product lines sold by a company.\n\nProduct mix refers to the total number of products sold in all product lines.\n\nLine extension refers to the adding of a new product to a line.\n\n\"Trading up or brand leveraging\" refers to adding a product of better quality to a product line than the other products in that line.\n\n\"Trading down\" refers to adding a product of lesser quality to a product line than the other products in that line.\n\nIf a line of products is sold with the same brand name, this is referred to as family branding.\nStrategy and decisions regarding a product line are usually incorporated in a high-level marketing plan addressing product line strategy, sales, channels, distribution channels, pricing and related issues.\nA product-line manager is responsible for a product line and supervises several product managers who are responsible for individual products within the line.\nProduct-line managers typically have the following responsibilities:\n- Expansion and composition of a product line\n- Evaluate the effects of product mixes on the profitability of other items in the line\n- Planning and allocation of resources to individual products in the line\nA product is normally associated with a brand strategy - manufacturer, private or generic.\n\n1. Manufacturer-  or 'national' branding in which the brand is assigned by the manufacturer of the Product.\n\n2. Private - or 'dealer' branding in which the brand is assigned by the retailer or wholesaler of the Product.\n\n3. Generic - in which the Product is not marked with any identification.   Generic brands are a means for manufacturers to increase profits by saving on advertising, packaging or other costs associated with manufacturer or private branding.\n\nA brand is name, term, sign, symbol or design or a combination of these which identify the goods or services and differentiate them from those of competitors'\n\nA Trade mark is a brand or some part of the brand that is given legal protection because it is capable of exclusive appropriation and representation.\n\nManufacturers can use their own brands (known as Manufacturers' brands) or brands of their distributors (Distributors' brands).\n\nManufacturers/ distributors use brand names for a variety of reasons ranging from simple identification purposes to having legal protection for unique features of the products from imitations.\n\nBrands help consumers recognize certain quality parameters. In some cases, brands are just used to endow the product with unique story and character which itself can be a basis for product differentiation.\n\nIndividual brands have their own identity and the corporate or common name is not used to promote its equity.\n\nIndividual branding requires more expensive advertising and brand extensive brand creation investments.  By extension, each new brand does not benefit from the positive perceptions of earlier brands.\n\nBy contrast, family branding has several advantages.\n\nEach new product is quickly associated with the other products and brand in terms of quality and benefits.\n\nReduced or eliminated time for name identification and advertising for name recognition purposes.",
								"DisplayFolderInfo": "{\"name\":\"Product\",\"colorCode\":\"#BD4B37\"}",
								"PrimaryKeys": "ProductId",
								"spark.sql.sources.provider": "csv"
							},
							"Retention": 0,
							"Temporary": false,
							"IsRewriteEnabled": false
						},
						"Source": {
							"Type": "SPARK"
						}
					},
					{
						"ActionType": "CREATE",
						"OldEntity": null,
						"NewEntity": {
							"Name": "SalesOrder",
							"EntityType": "TABLE",
							"Namespace": {
								"DatabaseName": "RetailDB"
							},
							"Description": "",
							"TableType": "EXTERNAL",
							"Origin": {
								"Type": "SPARK"
							},
							"StorageDescriptor": {
								"Columns": [
									{
										"Name": "SalesOrderID",
										"Description": "Unique identifier of Order",
										"OriginDataTypeName": {
											"TypeName": "long",
											"IsComplexType": false,
											"IsNullable": false,
											"Properties": {
												"HIVE_TYPE_STRING": "long"
											}
										}
									},
									{
										"Name": "OrderDate",
										"Description": "Date of the Order Created",
										"OriginDataTypeName": {
											"TypeName": "timestamp",
											"IsComplexType": false,
											"IsNullable": false,
											"Properties": {
												"TimestampFormat": "YYYY-MM-DD HH:MM:SS.fffffffff",
												"HIVE_TYPE_STRING": "timestamp"
											}
										}
									},
									{
										"Name": "LineItemID",
										"Description": "ID of the Individual Item",
										"OriginDataTypeName": {
											"TypeName": "long",
											"IsComplexType": false,
											"IsNullable": false,
											"Properties": {
												"HIVE_TYPE_STRING": "long"
											}
										}
									},
									{
										"Name": "CustomerID",
										"Description": "Customer Reference",
										"OriginDataTypeName": {
											"TypeName": "long",
											"IsComplexType": false,
											"IsNullable": false,
											"Properties": {
												"HIVE_TYPE_STRING": "long"
											}
										}
									},
									{
										"Name": "ProductID",
										"Description": "Product Reference",
										"OriginDataTypeName": {
											"TypeName": "long",
											"IsComplexType": false,
											"IsNullable": false,
											"Properties": {
												"HIVE_TYPE_STRING": "long"
											}
										}
									},
									{
										"Name": "Quantity",
										"Description": "Order Quantity",
										"OriginDataTypeName": {
											"TypeName": "long",
											"IsComplexType": false,
											"IsNullable": false,
											"Properties": {
												"HIVE_TYPE_STRING": "long"
											}
										}
									}
								],
								"Format": {
									"InputFormat": "org.apache.hadoop.mapred.SequenceFileInputFormat",
									"OutputFormat": "org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat",
									"FormatType": "csv",
									"SerializeLib": "org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe",
									"Properties": {
										"path": "abfss://files@datalake8heknbv.dfs.core.windows.net/RetailDB/SalesOrder/salesorder.csv",
										"delimiter": ",",
										"firstRowAsHeader": "false",
										"multiLine": "false",
										"serialization.format": "1",
										"FormatTypeSetToDatabaseDefault": false,
										"header": "false"
									}
								},
								"Source": {
									"Provider": "ADLS",
									"Location": "abfss://files@datalake8heknbv.dfs.core.windows.net/RetailDB/SalesOrder/salesorder.csv",
									"Properties": {
										"LinkedServiceName": "synapse8heknbv-WorkspaceDefaultStorage",
										"LocationSetToDatabaseDefault": false
									}
								},
								"Properties": {
									"textinputformat.record.delimiter": ",",
									"compression": "{\"type\":\"None\",\"level\":\"optimal\"}",
									"derivedModelAttributeInfo": "{\"attributeReferences\":{}}"
								},
								"Compressed": false,
								"IsStoredAsSubdirectories": false
							},
							"Properties": {
								"Description": "",
								"DisplayFolderInfo": "{\"name\":\"Others\",\"colorCode\":\"\"}",
								"PrimaryKeys": "SalesOrderID",
								"spark.sql.sources.provider": "csv"
							},
							"Retention": 0,
							"Temporary": false,
							"IsRewriteEnabled": false
						},
						"Source": {
							"Type": "SPARK"
						}
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Datawarehouse_poc')]",
			"type": "Microsoft.Synapse/workspaces/sqlPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"collation": "SQL_Latin1_General_CP1_CI_AS",
				"maxSizeBytes": 263882790666240,
				"annotations": []
			},
			"dependsOn": [],
			"location": "eastus2"
		}
	]
}